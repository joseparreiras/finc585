\begin{solution}
\begin{enumerate}[label = (\alph*)]
    \item Since
    \[
        \mb y_t = \alpha + \beta \mb x + \mb u = \theta\mb X + \mb u
    \]
    Where \(\mb X = \begin{bmatrix} \mb 1_T & \mb x \end{bmatrix}\) and \(\theta = \bp{\alpha , \beta}\). Then the usual OLS estimator of \(\theta\) is given by
    \[
        \wh \theta = \left(\mb X^\prime \mb X \right)^{-1}\mb X^\prime \mb y
    \]
    Therefore \(\beta = \begin{bmatrix} 0 & 1 \end{bmatrix}\theta \) and
    \begin{align*}
        \wh\beta & = \begin{bmatrix} 0 & 1 \end{bmatrix}\wh\theta \\
        & = \begin{bmatrix} 0 & 1 \end{bmatrix}\bs{\theta + \bp{\mb X^\prime\mb X}^{-1}\mb X \mb u} \\
        & = \beta + \begin{bmatrix} 0 & 1 \end{bmatrix} \bp{\mb X^\prime \mb X}^{-1}\mb X^\prime \mb u
    \end{align*}

    \item\label{item:u_decomp_solution} Since \(u_t\) and \(v_t\) are jointly normal, then we can write that 
    \[
        u_t = \gamma v_t + e_t
    \]
    where \(e_t\) is uncorrelated with \(v_t\) and \(\gamma\) is given by the OLS coefficient \(\gamma = \frac{\sigma_{uv}}{\sigma_v^2}\) . Therefore we can rewrite the equation from the previous item as:
    \begin{align*}
        \wh\beta & = \beta + \begin{bmatrix} 0 & 1 \end{bmatrix}\bp{\mb X^\prime \mb X}^{-1}\mb X^\prime \mb u \\
        & = \beta + \begin{bmatrix} 0 & 1 \end{bmatrix}\bp{\mb X^\prime \mb X}^{-1}\mb X^\prime  \bp{\gamma \mb v + \mb e} \\
        & = \beta + \frac{\sigma_{uv}}{\sigma_v^2}\begin{bmatrix} 0 & 1 \end{bmatrix}\bp{\mb X^\prime \mb X}^{-1} \mb X^\prime v + \begin{bmatrix} 0 & 1 \end{bmatrix}\bp{\mb X^\prime \mb X}^{-1}\mb X^\prime \mb e
    \end{align*}
        
    \item\label{item:rho_decomp_solution} Follows from the same logic as in the first item.
    \item Combining item \ref{item:u_decomp_solution} and \ref{item:rho_decomp_solution} we can write that
    \begin{align*}
        \wh\beta-\beta = \frac{\sigma_{uv}}{\sigma_v^2}\begin{bmatrix} 0 & 1\end{bmatrix}\bp{\wh\rho-\rho}+ \begin{bmatrix}0 & 1 \end{bmatrix} \bp{\mb X^\prime\mb X}^{-1}\mb X \mb e
    \end{align*}
    Then using the assumption that \(e_t\) is independent from \(\mathcal{F}_{t-1}\) (the information from the previous period, which contains the lagged \(\mb X\)), we can take the conditional expectation 
    \begin{align*}
        \E[t-1]\bs{\wh\beta-\beta} & = \frac{\sigma_{uv}}{\sigma_v^2}\begin{bmatrix} 0 & 1\end{bmatrix}\E[t-1]\bs{\wh\rho-\rho} + \begin{bmatrix}0 & 1 \end{bmatrix} \bp{\mb X^\prime\mb X}^{-1}\mb X \E[t-1]\bs{\mb e} \\
        \E[t-1]\bs{\wh\beta-\beta} & = \frac{\sigma_{uv}}{\sigma_v^2}\begin{bmatrix} 0 & 1\end{bmatrix}\E[t-1]\bs{\wh\rho-\rho} \\
        \E{\wh\beta-\beta} & = \frac{\sigma_{uv}}{\sigma_v^2}\E{\wh\rho-\rho}
    \end{align*}
    Using the  expression from \citet{kendall1954note} that implies
    \[
        \E{\wh\beta-\beta}\approx -\frac{\sigma_{uv}}{\sigma_v^2}\frac{1+3p}{T}
    \]
    Using the proposed values of \(T = 50\), \(\rho = 0.94\) and \(\frac{\sigma_{uv}}{\sigma_v^2} = -1\) we would have a bias of \(\wh\beta\) equal to \(0.0764\). Therefore, just from the persistence of the regressor, our estimator would be biased.

    \item This results suggest that the persistence of the regressors can be a real enemy when estimating a predictive regression. As we know, the more persistent the \(x\) (\(\rho \to 1\)) the harder it gets to estimate this parameter and the higher the estimation bias of the AR coefficients. This feeds back to the predictive equation by also increasing the bias of the estimated \(\beta\). This is a problem because we are interested in the predictive power of the regressor, but the more persistent it is, the harder it gets to estimate it and the more biased the estimated \(\beta\) will be. 
    The interpretation behind this is that, as \(x\) gets more persistent, shocks in the predictor tend to last longer, which makes it harder to estimate the real impact of \(x\) on \(y\).

    \item To start, I fix \(\beta = 1\) and \(\alpha = 0\). The errors \(u\) and \(v\) are set to have unit variance. Then I simulate the model varying the levels of \(T, \sigma_{uv}, \rho\):
    \begin{align*}
        T & \in \bc{50, 100, 150, \dots, 1000} \\
        \sigma_{uv} & \in \bc{-0.9, -0.8, \dots, 0.8, 0.9} \\
        \rho & \in \bc{0, 0.3, 0.5, 0.9, 0.95, 0.99}
    \end{align*}
    Every model is simulated 10.000 times and I calculate the average bias of \(\wh\beta\) among these simulations. The results are shown in Figure~\ref{fig:bias} which displays the absolute level of the bias for each level of \(\rho\), \(\sigma_{uv}\) and \(T\). As we can see, the bias dies out quickly with T, but it is very sensitive to the level of persistence of the regressor. The higher the persistence, the higher the bias and the longer it takes to die out. The same happens with the covariance between the errors. The higher the covariance, the higher the bias and the longer it takes to die out. For higher levels of \(\rho\) such as 0.99, we see that the bias is still big for sample sizes around 150, which can sometimes be considered big.
    \begin{figure}[!htbp]
        \centering
        \includegraphics[width = \textwidth]{bias.jpg}
        \caption{Bias of the OLS estimator under different levels of \(\rho\), \(\sigma_{uv}\) and \(T\)}
        \label{fig:bias}
    \end{figure}
\end{enumerate}
\end{solution}